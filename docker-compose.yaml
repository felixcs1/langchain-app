version: "3.8"
services:
  ollama:
    container_name: ollama
    image: ollama/ollama
    volumes:
      - ./models:/root/.ollama
    # Only to check ollama is running on local host
    ports:
      - 11434:11434
  langserve:
    container_name: langserve
    build:
      context: ./backend
      dockerfile: Dockerfile
    volumes:
      - ./:/src
    ports:
      - "8080:8080"
    depends_on:
      - ollama
  react:
    container_name: react
    build:
      context: ./frontend
      dockerfile: Dockerfile
    volumes:
      - ./frontend/src:/app/src
    ports:
      - "80:80"
    stdin_open: true
    tty: true
    depends_on:
      - langserve


# To get new model into local folder
# docker run --rm -d -p 11434:11434 --name ollama --network ollama-net -v $(pwd)/models:/root/.ollama ollama/ollama
# docker exec -it  ollama ollama run orca-mini:3b

# Run with commands
# docker network create ollama-net
# docker run --rm -d -p 11434:11434 --name ollama --network ollama-net -v $(pwd)/models:/root/.ollama ollama-run
# docker run --rm  -p 8080:8080 --name langserve --network ollama-net langserve
